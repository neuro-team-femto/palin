---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
    collapse = TRUE,
    cache = TRUE,
    warning = FALSE,
    message = FALSE,
    comment = "#>",
    fig.path = "man/figures/README-",
    out.width = "100%"
    )
```

# PALIN: Convert your PAs into INs

<!-- badges: start -->
<!-- badges: end -->

The goal of `palin` is to provide utilities for working with reverse correlation data. It can be used, for instance, to estimate internal noise (IN) from empirical percentages of agreement (PA) in such tasks.

## Installation

You can install the development version of `palin` from GitHub with:

```{r, eval = FALSE}
install.packages("remotes")

remotes::install_github(
    repo = "https://github.com/neuro-team-femto/palin/r",
    dependencies = TRUE
    )
```

## Usage

### Computing kernels

Below we compute kernels using various methods.

```{r kernels}
library(patchwork)
library(tidyverse)
library(palin)

# importing some reverse correlation data (self-produced speech)
data(self_voice)
head(self_voice)

# computing the kernel via the difference method
diff_kernels <- computing_kernel(data = self_voice, method = "difference")
head(diff_kernels)

# computing the kernel using a GLM
glm_kernels <- computing_kernel(data = self_voice, method = "glm")
head(glm_kernels)

# plotting everything (see the doc via ?plot.kernel)
plot(diff_kernels, normalisation_method = "kernel_gain") +
    plot(glm_kernels, normalisation_method = "kernel_gain")
```

### Estimating internal noise

Computing response bias and internal noise from the percentage of agreement.

```{r sdt-example}
# computing the percentage of agreement and the percentage of choosing the first
# stimulus in the double-pass trials
self_voice |>
    filter(participant == unique(self_voice$participant)[1]) |>
    response_consistency() |>
    select(participant, double_pass_prop_agree, double_pass_prop_first) |>
    distinct()

# estimating internal noise for this  participant
df <- data.frame(prop_agree = 0.66, prop_first = 0.34, ntrials = 100)

# fitting the SDT model to these data (using DEoptim and all available cores)
# the value represents the value of the cost function (log-MSE)
fit_results <- sdt_fitting(data = df, maxit = 100)
summary(fit_results)
```

### Drift diffusion modelling

Fitting the drift diffusion model (DDM) to the responses and distributions of RTs (after removing the last double-pass block).

```{r ddm-example}
library(fddm)

# reshaping the data
df <- self_voice |>
    # keeping only the first participant
    filter(participant == unique(self_voice$participant)[1]) |>
    # removing the last double-pass block
    filter(block < max(block) ) |>
    # keeping only the relevant columns
    select(participant, trial, response, RT) |>
    # removing duplicated rows
    distinct() |>
    # reshaping the resp variable as indicating int1 or int2
    mutate(
        resp = if_else(first(response) == 1, 0, 1),
        .by = c(participant, trial)
        ) |>
    distinct() |>
    # reshaping the resp variable
    mutate(resp = factor(ifelse(test = response == 0, yes = "lower", no = "upper") ) ) |>
    # removing extreme RTs
    filter(RT > 0.05 & RT < 2)

# plotting the RT distribution
# hist(df$RT, breaks = "FD")

# fitting the full DDM (pars are a, v, t0, w, sv)
# parameters are the threshold separation, drift rate, non-decision time,
# relative starting point, and inter-trial variability of drift rate
# verbose = 20 means that we want to print progress every 20 iterations
ddm_fit <- ddm_fitting(
    rt = df$RT,
    resp = df$resp,
    method = "DEoptim",
    maxit = 1e3
    )
summary(ddm_fit)

# comparing to the fit with ddm()
fddm_fit <- fddm::ddm(
    drift = RT + resp ~ 1,
    boundary = ~ 1,
    ndt = ~ 1,
    bias = ~ 1,
    sv = ~ 1,
    optim = "nlminb",
    args_optim = list(
        lo_bds = c(-5, 0, 0, 0, 0),
        up_bds = c(+5, 5, 5, 1, 5)
        ),
    data = df
    )

# retrieving a summary
fddm_fit$coefficients
fddm_fit$loglik
```

### Computing continous metrics of response consistency

```{r consistency}
# computing average consistency per participant and block
consistency <- response_consistency(
    data = self_voice,
    # method can be one of c("template_distance", "kernel_similarity", "intercept")
    method = "template_distance",
    double_pass = TRUE
    )

# plotting it
consistency %>%
    pivot_longer(cols = avg_consistency:weighted_consistency) %>%
    ggplot(aes(x = block, y = value, colour = name) ) +
    geom_line(
        aes(group = interaction(participant, name) ),
        linewidth = 0.5,
        alpha = 0.5,
        show.legend = FALSE
        ) +
    stat_summary(
        geom = "line",
        fun = median, linewidth = 1,
        show.legend = FALSE
        ) +
    facet_wrap(~name, ncol = 1) +
    theme_bw(base_size = 12, base_family = "Open Sans") +
    labs(x = "Block number", y = "Average consistency")
```
